{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f9f909",
   "metadata": {},
   "source": [
    "# Web page & policy tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e00d5",
   "metadata": {},
   "source": [
    "## Table Of Content:\n",
    "* [Install packages](#first-bullet)\n",
    "* [Read in data](#second-bullet)\n",
    "* [Preprocessing](#third-bullet)\n",
    "* [Installing word to vec model](#third-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678468f8",
   "metadata": {},
   "source": [
    "## 1) Install packages <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b88efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#read in data\n",
    "import os\n",
    "import json\n",
    "\n",
    "#preprocessing\n",
    "import re, string\n",
    "\n",
    "#tagging webpages to words\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f5e9f",
   "metadata": {},
   "source": [
    "## 2) Read in data <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa4561",
   "metadata": {},
   "source": [
    "Read in web pages as json file, read in policy codes in csv format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf16f2b",
   "metadata": {},
   "source": [
    "### Read in web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"html_data_2.json\", encoding=\"utf-8\") as jsonFile:\n",
    "    jsonObject = json.load(jsonFile)\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4aff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jsonObject['html_list']\n",
    "\n",
    "df_webpages = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_webpages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_webpages.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286d16f",
   "metadata": {},
   "source": [
    "### Read in policy codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policy_codes = pd.read_csv('policy_codes.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policy_codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policy_codes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf86f0",
   "metadata": {},
   "source": [
    "## 3) Preprocessing <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aebe03",
   "metadata": {},
   "source": [
    "Preprocessing of web pages and policy codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = df_webpages.loc[:,'html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293eefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648e609",
   "metadata": {},
   "source": [
    "### Removing punctuations, numbers, to lower,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b74d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [preprocess_text(t) for t in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca28b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_webpages['text_preprocessed']=data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e5809",
   "metadata": {},
   "source": [
    "## 4) Installing word to vec model <a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b0617",
   "metadata": {},
   "source": [
    "Word to vec model from: http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a84b16",
   "metadata": {},
   "source": [
    "### From CoNLL17 corpus to gensim word2vec (ONLY RUN 1 TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''col_names = [\"word\"]\n",
    "for i in range(0,100):\n",
    "    col_names.append(\"vec\" + str(i))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedded_dic = pd.read_csv('model.txt', skiprows=1, sep=' ', encoding='latin-1', names = col_names, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedded_dic['word'] = embedded_dic['word'].str.encode('latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e53ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedded_dic['word'] = embedded_dic['word'].str.decode('utf-8', errors='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade16593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedded_dic.index = embedded_dic['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedded_dic = embedded_dic.drop(columns = 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2426ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''np.savetxt('embedded_dic_english.txt', embedded_dic.reset_index().values, \n",
    "           delimiter=\" \", \n",
    "           header=\"{} {}\".format(len(embedded_dic), len(embedded_dic.columns)),\n",
    "           comments=\"\",\n",
    "           fmt=[\"%s\"] + [\"%.18e\"]*len(embedded_dic.columns), encoding = 'utf-8')''' #save the model as a model you can use with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6221e32",
   "metadata": {},
   "source": [
    "### Create similarity measure between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94780eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopW = set(stopwords.words('english'))\n",
    "punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "class DocSimV1(object):\n",
    "    def __init__(self, w2v_model , stopwords=stopW , remove_punctuation_map=punctuation_map):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords\n",
    "        self.remove_punctuation_map = punctuation_map\n",
    "\n",
    "    def vectorize(self, doc):\n",
    "        \"\"\"Identify the vector values for each word in the given document\"\"\"\n",
    "        doc = doc.lower()\n",
    "        words = [w.translate(punctuation_map) for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "\n",
    "        # Assuming that document vector is the mean of all the word vectors\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n",
    "        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "        if np.isnan(np.sum(csim)):\n",
    "            return 0\n",
    "        return csim\n",
    "\n",
    "    def calculate_similarity(self, source_doc, target_docs=[], threshold=0):\n",
    "        \"\"\"Calculates & returns similarity scores between given source document & all\n",
    "        the target documents.\"\"\"\n",
    "        if isinstance(target_docs, str):\n",
    "            target_docs = [target_docs]\n",
    "\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        results = []\n",
    "        for doc in target_docs:\n",
    "            target_vec = self.vectorize(doc)\n",
    "            sim_score = self._cosine_sim(source_vec, target_vec)\n",
    "            if sim_score > threshold:\n",
    "                results.append({\n",
    "                    'score' : sim_score,\n",
    "                    'doc' : doc\n",
    "                })\n",
    "            # Sort results by score in desc order\n",
    "            #results.sort(key=lambda k : k['score'] , reverse=True)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f60c0",
   "metadata": {},
   "source": [
    "### Practise to see if model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb89ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('embedded_dic_english.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DocSimV1(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.calculate_similarity(\"passport\", [\"travel airport documents\",\"travel\",\"sugar\", \"Identity card\", \"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480696c3",
   "metadata": {},
   "source": [
    "## Tagging webpages to policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebb4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags(org_data, policy_codes,threshold):\n",
    "    \n",
    "    index_text = org_data.columns.get_loc(\"text_preprocessed\")\n",
    "    main_policy_codes=policy_codes[\"Policy_code\"][policy_codes[\"Policy_code\"].str.len() == 1]\n",
    "    \n",
    "    all_tags =[]\n",
    "    policy_codes_list = []\n",
    "\n",
    "    for main_code in main_policy_codes:\n",
    "        temp_one_policy = policy_codes[policy_codes.Policy_code.str.startswith(main_code)]\n",
    "        sub_policy_codes=temp_one_policy[\"Policy_code\"][temp_one_policy[\"Policy_code\"].str.len() ==2]\n",
    "        policy_codes_list.append(sub_policy_codes.values.tolist())       \n",
    "        \n",
    "        sub_policy_list = []\n",
    "\n",
    "        for sub_policy_code in sub_policy_codes:\n",
    "            sub_policy_tag = policy_codes[policy_codes[\"Policy_code\"]==sub_policy_code]\n",
    "                       \n",
    "            sub_policy_tag = sub_policy_tag[\"relevant_tags\"].str.split(',', expand=True)\n",
    "                        \n",
    "            sub_policy_list.append(sub_policy_tag.values.tolist()[0])\n",
    "            \n",
    "        all_tags.append(sub_policy_list)            \n",
    "               \n",
    "    #print(policy_codes_list)                \n",
    "    \n",
    "    temp = copy.deepcopy(org_data)\n",
    "    \n",
    "    ds = DocSimV1(word_vectors)\n",
    "    \n",
    "    for code in main_policy_codes:\n",
    "        temp[code]=0 \n",
    "           \n",
    "    temp['tags'] = \"\"\n",
    "    temp['subtags'] = \"\"\n",
    "    temp['tag_word'] =\"\"\n",
    "    \n",
    "    index_tags = temp.columns.get_loc(\"tags\")  \n",
    "    index_subtags = temp.columns.get_loc(\"subtags\")\n",
    "\n",
    "    index_tagwords = temp.columns.get_loc(\"tag_word\")\n",
    "\n",
    "    #dubbelcheck if the words in the brackets are the words we want to match with\n",
    "    \n",
    "    # combine the categories in a list\n",
    "      \n",
    "    tags_en = []\n",
    "    tags_en = all_tags\n",
    "    #print(tags_en)\n",
    "          \n",
    "    #define the names of the subcategories\n",
    "          \n",
    "    subtags_names = []\n",
    "    subtags_names = policy_codes_list\n",
    "    #print(subtags_names)\n",
    "    \n",
    "\n",
    "    #splitting into two word comparison\n",
    "    for i in range(0,len(temp)): # doc per doc\n",
    "        words = temp.iloc[i,index_text].split()\n",
    "        word_filters = [] #contains all three-words of a document \n",
    "        \n",
    "           \n",
    "        for w in range(0,len(words)-1):          \n",
    "            word_filters.append(words[w] + \" \" + words[w+1])\n",
    "        \n",
    "        for word_filter in word_filters:\n",
    "            \n",
    "            for t in range(0,len(tags_en)): # every main tag\n",
    "                for subt in range(0,len(tags_en[t])): # for every subtag\n",
    "                    sim_scores = ds.calculate_similarity(word_filter, tags_en[t][subt]) # compute similarity between preprocessed text and tag\n",
    "                    scores = [sim_scores[k].get('score') for k in range(0,len(sim_scores))]\n",
    "                    col_pos = index_text + 1 +t\n",
    "                    score_max = max(scores, default=0)\n",
    "                \n",
    "                    if score_max > temp.iloc[i,col_pos]:\n",
    "                        temp.iloc[i,col_pos] = float(score_max) # add max value of match under the tag variable\n",
    "\n",
    "                    if score_max >=threshold:\n",
    "                        index_max = np.argmax(scores)\n",
    "                                                                       \n",
    "                        if temp.columns[index_text + 1 + t] not in temp.iloc[i,index_tags]:\n",
    "                            temp.iloc[i,index_tags] = temp.iloc[i,index_tags] + temp.columns[index_text + 1 +t] + \", \" #add main tag\n",
    "                                         \n",
    "                        if subtags_names[t][subt] not in temp.iloc[i,index_subtags]:\n",
    "                            temp.iloc[i,index_subtags] = temp.iloc[i,index_subtags] + subtags_names[t][subt] + \", \" #add sub tag\n",
    "                        \n",
    "                        if tags_en[t][subt][index_max] not in temp.iloc[i,index_tagwords]:\n",
    "                            temp.iloc[i,index_tagwords] = temp.iloc[i,index_tagwords] + tags_en[t][subt][index_max] + \", \" #add tag word                           \n",
    "       \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = create_tags(df_webpages[222:223], df_policy_codes, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43931cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccaf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a8090",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('webpages_policies_tagged.csv', index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3516b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
